{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import Client, SSHCluster\n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline AHN1 Workflow - Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Choose whether you want to run all input files or run the only input files listed in `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = pathlib.Path('/data/local/home/eecolidar_webdav/02_UvA/')\n",
    "\n",
    "# path to retiled files \n",
    "path_input = path_root / 'YShi/AHN1/Retiled'\n",
    "\n",
    "# path to normalized files\n",
    "path_output = path_input.parent / 'normalized'\n",
    "\n",
    "run = 'from_file' # 'all', 'from_file'\n",
    "filename = 'normalize_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'from_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 37715 tiles\n",
      "Normalize: 8989 tiles\n"
     ]
    }
   ],
   "source": [
    "tiles = [el for el in path_input.iterdir() if el.match('tile_*_*/')]\n",
    "print('Found: {} tiles'.format(len(tiles)))\n",
    "if run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        tiles_read = json.load(f)\n",
    "    tiles_read = [path_input/f for f in tiles_read]\n",
    "    # check whether all files are available on dCache\n",
    "    assert all([f in tiles for f in tiles_read]), f'Some of the tiles in {filename} are not in input dir'\n",
    "    tiles = tiles_read\n",
    "print('Normalize: {} tiles'.format(len(tiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for the macro-pipeline calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at: tcp://145.100.59.123:8786\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.59:45041'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.59:41601'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.182:40825'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.182:38169'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.187:42841'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.48:37241'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.123:44243'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.187:35421'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.84:39871'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.123:35483'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.48:40993'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.27:36103'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.27:45803'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.84:40623'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.115:37275'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.115:45407'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.197:36359'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.197:40585'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.118:39651'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.118:34857'\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.59:46365\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.182:34725\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.27:35027\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.48:39329\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.187:40469\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.123:40023\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.197:41021\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.84:37539\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.115:35353\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.118:41699\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>SSHCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='/proxy/8787/status' target='_blank'>/proxy/8787/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "SSHCluster('tcp://145.100.59.123:8786', workers=18, threads=18)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_tmp = pathlib.Path('/data/local/tmp')\n",
    "\n",
    "nprocs_per_node = 2  \n",
    "\n",
    "# start the cluster\n",
    "scheduler_node = 'node1'\n",
    "hosts = [f'node{i}' for i in range(1, 11)]\n",
    "cluster = SSHCluster(hosts=[scheduler_node] + hosts, \n",
    "                     connect_options={'known_hosts': None, \n",
    "                                      'username': 'ubuntu', \n",
    "                                      'client_keys': '/home/ubuntu/.ssh/id_rsa'},\n",
    "                     worker_options={'nthreads': 1, \n",
    "                                     'nprocs': nprocs_per_node,\n",
    "                                     'memory_limit': 0,\n",
    "                                     'local_directory': local_tmp/'dask-worker-space'}, \n",
    "                     scheduler_options={'dashboard_address': '8787'})\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalize the point heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the normalization pipeline\n",
    "normalization_input = {\n",
    "    'setup_local_fs': {'input_folder': path_input.as_posix(),\n",
    "                       'output_folder': path_output.as_posix()},\n",
    "    'load': {'attributes': 'all'},\n",
    "    # Filter out artifically high points - give overflow error when writing \n",
    "    'apply_filter': {'filter_type':'select_below',\n",
    "                     'attribute': 'z',\n",
    "                     'threshold': 10000.},  # remove non-physically heigh points\n",
    "    'normalize': 1,\n",
    "    'clear_cache' : {},\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('normalize.json', 'w') as f:\n",
    "    json.dump(normalization_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "for tile in tiles:\n",
    "    dp = DataProcessing(tile.name, label=tile.name)\n",
    "    normalization_input_ = copy.deepcopy(normalization_input)\n",
    "    normalization_input_['export_point_cloud'] = {'filename': '{}.laz'.format(tile.name),\n",
    "                                                  'overwrite': True}\n",
    "    dp.config(normalization_input_)\n",
    "    macro.add_task(dp)\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='normalize.out')\n",
    "\n",
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('normalize_failed.json', 'w') as f:\n",
    "        json.dump([pip.label for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel all jobs and restart the notebook\n",
    "\n",
    "Copy and paste these lines in a separate Python shell. If the Dask dashboard shows that some tasks are still queued to be processed, run the lines again - this should clear the scheduler up and give back control to the current notebook. Normally proceed to terminate the cluster and restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, Future\n",
    "client = Client('tcp://145.100.59.123:8786')\n",
    "futures = [Future(key) for key in client.who_has().keys()]\n",
    "client.cancel(futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
