{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from distutils import dir_util             \n",
    "from dask.distributed import LocalCluster, SSHCluster, Client, as_completed  \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "def last_modified(opts, remote_path):\n",
    "    info = get_info_remote(get_wdclient(opts), remote_path.as_posix())\n",
    "    format_ = '%a, %d %b %Y %H:%M:%S GMT'\n",
    "    return datetime.datetime.strptime(info['modified'], format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline AHN4 Workflow - Retiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    " Choose whether you want to i) run all input files, ii) run the only input files listed in `filename`, or iii) run the input that was updated since the last workflow run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remote_path_root = pathlib.Path('/project/lidarac/Data/AHN4')\n",
    "\n",
    "# path to raw LAZ files \n",
    "remote_path_input = remote_path_root / 'splitted'\n",
    "\n",
    "# path where to copy retiled LAZ files\n",
    "remote_path_output = remote_path_input.parent / 'retiled'\n",
    "\n",
    "run = 'from_file' # 'all', 'updated', 'from_file'\n",
    "filename = 'retile_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'updated', 'from_file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Connection to Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_files = [f for f in listdir(remote_path_input) if isfile(join(remote_path_input, f))\n",
    "             if f.lower().endswith('.LAZ')]\n",
    "print('Found: {} LAZ files'.format(len(laz_files)))\n",
    "if run == 'updated':\n",
    "    laz_files = [f for f in laz_files]\n",
    "elif run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        laz_files_read = json.load(f)\n",
    "    # check whether all files are available on dCache\n",
    "    assert all([f in laz_files for f in laz_files_read]), f'Some of the files in {filename} are not in remote dir'\n",
    "    laz_files = laz_files_read\n",
    "print('Retrieve and retile: {} LAZ files'.format(len(laz_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for the macro-pipeline calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.0.1.12:43843\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Re-tiling\n",
    "\n",
    "The raw point cloud files are downloaded and retiled to a regular grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# details of the retiling schema - for the Netherlands\n",
    "grid = {\n",
    "    'min_x': -113107.81,\n",
    "    'max_x': 398892.19,\n",
    "    'min_y': 214783.87,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 512\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': remote_path_input.as_posix(),\n",
    "        'output_folder': remote_path_output.as_posix()\n",
    "    },\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [Retiler(file).config(retiling_input) for file in laz_files]\n",
    "macro.set_labels([os.path.splitext(file)[0] for file in laz_files])\n",
    "\n",
    "macro.setup_cluster(cluster=\"tcp://10.0.1.12:43843\")\n",
    "\n",
    "# run! \n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='retile.out')\n",
    "\n",
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('retile_failed.json', 'w') as f:\n",
    "        json.dump([pip.label + '.LAZ' for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('retile_failed.json', 'w') as f:\n",
    "        json.dump([pip.label + '.LAZ' for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel all jobs and restart the notebook\n",
    "\n",
    "Copy and paste these lines in a separate Python shell. If the Dask dashboard shows that some tasks are still queued to be processed, run the lines again - this should clear the scheduler up and give back control to the current notebook. Normally proceed to terminate the cluster and restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, Future\n",
    "client = Client('tcp://145.100.59.123:8786')\n",
    "futures = [Future(key) for key in client.who_has().keys()]\n",
    "client.cancel(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
