{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "from dask.distributed import Client, SSHCluster\n",
    "from laserfarm import Retiler, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline AHN4 Workflow - GeoTIFF Export (Powerline Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    "Choose whether you want to run all input files or run the only input files listed in `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_root = pathlib.Path('/project/lidarac/Data/AHN4')\n",
    "\n",
    "# path to normalized files \n",
    "path_input = path_root / 'targets_powerline'\n",
    "\n",
    "# path to targets\n",
    "path_output = path_input.parent / 'geotiff_powerline'\n",
    "\n",
    "run = 'all'  # 'all', 'from_file'\n",
    "#filename = 'powerline_geotiff_export_non-ground_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'from_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [el for el in path_input.iterdir() if not el.match('tile_*_*.log')]\n",
    "print('Found: {} features'.format(len(features)))\n",
    "if run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        features_read = json.load(f)\n",
    "    features_read = [path_input/f for f in features_read]\n",
    "    # check whether all files are available on dCache\n",
    "    assert all([f in features for f in features_read]), f'Some of the features in {filename} are not in input dir'\n",
    "    features = features_read\n",
    "print('Extract geotiffs for: {} features'.format(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://10.0.1.12:33245\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoTIFF Export\n",
    "\n",
    "Export the rasterized features from the target grid to GeoTIFF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output handle: AHN4 dataset, features, target grid spacing 10m, normalization grid spacing 1m, all points\n",
    "output_handle = 'AHN4_powerline_mask_'\n",
    "\n",
    "# setup input dictionary to configure the geotiff export pipeline\n",
    "geotiff_export_input_nonground = {\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {'xSub': 1, 'ySub': 1},\n",
    "    'create_subregion_geotiffs': {'output_handle': output_handle},\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('geotiff_export_input_non-ground.json', 'w') as f:\n",
    "    json.dump(geotiff_export_input_nonground, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "for feature in features:\n",
    "    gw = GeotiffWriter(bands=feature.name, label=feature.name)\n",
    "    geotiff_export_input_nonground_ = copy.deepcopy(geotiff_export_input_nonground)\n",
    "    geotiff_export_input_nonground_['setup_local_fs'] = {\n",
    "        'input_folder': feature.as_posix(),\n",
    "        'output_folder': path_output.as_posix()\n",
    "    }\n",
    "    gw.config(geotiff_export_input_nonground_)\n",
    "    macro.add_task(gw)\n",
    "\n",
    "macro.setup_cluster(cluster=\"tcp://10.0.1.12:33245\")\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and write name of failed pipelines to file\n",
    "macro.print_outcome(to_file='geotiff_export_non-ground.out')\n",
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('geotiff_export_non-ground_failed.json', 'w') as f:\n",
    "        json.dump([pip.label for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "macro.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel all jobs and restart the notebook\n",
    "\n",
    "Copy and paste these lines in a separate Python shell. If the Dask dashboard shows that some tasks are still queued to be processed, run the lines again - this should clear the scheduler up and give back control to the current notebook. Normally proceed to terminate the cluster and restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, Future\n",
    "client = Client('tcp://145.100.59.123:8786')\n",
    "futures = [Future(key) for key in client.who_has().keys()]\n",
    "client.cancel(futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
